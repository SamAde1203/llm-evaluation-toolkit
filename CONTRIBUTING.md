# Contributing to LLM Evaluation Toolkit

Thank you for your interest in contributing!  
This project welcomes improvements of all kinds ‚Äî bug fixes, new metrics, documentation updates, and feature proposals.

Please take a moment to read the guidelines below before contributing.

---

## üêõ Reporting Bugs

Before submitting a bug report:

1. Check **existing Issues** to avoid duplicates.
2. Use the **Bug Report Template**.
3. Include:
   - Steps to reproduce the issue
   - Full error traceback
   - OS and Python version
   - Example input/output if relevant

Bug reports with reproducible steps are resolved much faster.

---

## üí° Suggesting Features

We love suggestions!  
When proposing a feature:

1. Search existing feature requests first.
2. Clearly explain:
   - **Use case**
   - **Why it matters**
   - **How it benefits LLM evaluation**
3. Optional: provide a minimal code example or reference implementation.

Not every idea can be implemented, but all are considered.

---

## üîß Pull Request Workflow

To contribute code:

1. **Fork** the repository.
2. Create your feature branch:  
   ```bash
   git checkout -b feature/amazing-feature
Write clean, documented code.

Add or update unit tests for your changes.

Ensure all tests pass:

python -m pytest
Update README or docs if needed.

Submit a Pull Request with:

Clear description of changes

Reference to related issues

Screenshots/logs if applicable

PRs that follow this format get merged faster.

üìù Code Style
Please follow these guidelines:

Use type hints for all new functions.

Add docstrings (Google or NumPy style).

Keep functions modular and focused.

Use black for formatting (optional but recommended).

Do not commit large datasets or model files.

ü§ù Thank You!
Your contribution helps improve open-source LLM evaluation for everyone.
We deeply appreciate your time, creativity, and collaboration.