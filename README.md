# ğŸ§  LLM Evaluation Toolkit

[![Python](https://img.shields.io/badge/python-3.11%2B-blue)](https://www.python.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code Style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)











A modular, extensible, and reproducible framework for evaluating Large Language Models (LLMs) using multiple metrics, configurable pipelines, and automated reporting â€” built for AI researchers, ML engineers, AI safety teams, and product teams who require reliable evaluation at scale.

ğŸš€ Why This Toolkit?

Evaluating LLM outputs is one of the hardest problems in AI:

Different LLMs hallucinate, paraphrase, compress, or omit details

Classic metrics like BLEU/ROUGE fail to capture meaning

Different evaluators produce inconsistent results

Teams often rely on ad-hoc scripts that cannot be reproduced

The LLM Evaluation Toolkit provides:

âœ” A standardised evaluation pipeline
âœ” Multiple metrics for semantic + factual evaluation
âœ” Full YAML-based configuration
âœ” Detailed Markdown reports + visualisations
âœ” Reproducible scoring logic suitable for research and production
âœ” Lightweight integration with any LLM (GPT-4/Claude/Gemini/Llama/etc.)

This is the kind of framework used internally by OpenAI, Anthropic, DeepMind, Scale AI, and research labs â€” now available in an open-source form.

âœ¨ Features
ğŸ” Multi-Metric Evaluation

Exact Match

Fuzzy Matching (Levenshtein-based)

Keyword Coverage

Semantic Similarity (SentenceTransformers embeddings)

âš™ï¸ Configurable Pipeline

YAML/JSON configs

Adjustable metric weights

Threshold controls

Model selection for semantic similarity

ğŸ“Š Automated Report Generation

Markdown reports

Score breakdowns

Metric summaries

Visualisations (heatmaps, score histograms, etc.)

ğŸ§± Extensible Architecture

Add your own evaluation metric in minutes.

ğŸ§ª Dataset Tools

JSON/CSV dataset loaders

Synthetic dataset generation for experiments

ğŸ› ï¸ Installation
# Clone the repository
git clone https://github.com/SamAde1203/llm-evaluation-toolkit.git
cd llm-evaluation-toolkit

# Install dependencies
pip install -r requirements.txt

âš¡ Quick Start
from llm_eval.evaluator import LLMEvaluator

predictions = [
    "The capital of France is Paris.",
    "Water boils at 100Â°C."
]

references = [
    "Paris is the capital of France.",
    "Water boils at 100 degrees Celsius."
]

# Initialize evaluator
evaluator = LLMEvaluator()

# Evaluate batch
results = evaluator.evaluate_batch(predictions, references)

# Print summary
evaluator.print_summary()

# Save results
evaluator.save_results(results, "data/results/evaluation.json")

ğŸ“Š Evaluation Metrics
Metric	Description	Best For
Exact Match	Normalised string comparison	Factual Q&A
Fuzzy Match	Levenshtein similarity score	Typos / near-matches
Keyword Match	Factual token coverage	Content completeness
Semantic Similarity	Embedding-based cosine similarity	Paraphrasing, meaning
ğŸ—ï¸ Project Structure
llm-evaluation-toolkit/
â”œâ”€â”€ README.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ CODE_OF_CONDUCT.md
â”œâ”€â”€ LICENSE
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ default.yaml
â”‚   â””â”€â”€ advanced_config.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ evaluator.py
â”‚   â”œâ”€â”€ datasets.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ metrics/
â”‚       â”œâ”€â”€ correctness.py
â”‚       â”œâ”€â”€ relevance.py
â”‚       â””â”€â”€ safety.py
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ demo.py
â”‚   â”œâ”€â”€ comprehensive_demo.py
â”‚   â”œâ”€â”€ quickstart.ipynb
â”‚   â””â”€â”€ custom_metrics.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_metrics.py
â”‚   â”œâ”€â”€ test_evaluator.py
â”‚   â””â”€â”€ test_datasets.py
â”‚
â”œâ”€â”€ docs/images/
â”‚   â””â”€â”€ project_structure.png
â”‚
â”œâ”€â”€ reports/
â””â”€â”€ data/

ğŸ§© Advanced Configuration (YAML)
metrics:
  exact_match:
    enabled: true
    normalize: true

  fuzzy_match:
    enabled: true
    threshold: 0.7

  keyword_match:
    enabled: true

  semantic_similarity:
    enabled: true
    model_name: "all-MiniLM-L6-v2"

weights:
  exact_match: 0.3
  fuzzy_match: 0.2
  keyword_match: 0.2
  semantic_similarity: 0.3

output:
  save_results: true
  output_dir: "data/results/"
  generate_report: true

ğŸ§  System Architecture

<<<<<<< HEAD
![Project Structure](docs/images/project_structure.png)
=======
(Replace with a full architecture diagram if desired â€” I can generate one.)
>>>>>>> 4cacce4 (docs: rewrite README into world-class documentation)

ğŸ“ˆ Sample Output
Console Summary
Total Samples: 8
Metrics Used: exact_match, fuzzy_match, keyword_match, semantic_similarity

AGGREGATE SCORES:
semantic_similarity : 0.810 (Â±0.209)
keyword_match       : 0.473 (Â±0.248)
exact_match         : 0.000
fuzzy_match         : 0.125
overall             : 0.363

Example Visualisation

ğŸ§ª Running Tests
python -m pytest tests/

ğŸ”® Roadmap
Planned Enhancements:

LLM-as-Judge evaluation (GPT-4 / Claude / Gemini)

BLEU / ROUGE / METEOR support

Toxicity & safety signal detection

Web dashboard UI

API endpoints for cloud-based evaluation

HuggingFace integration

Model benchmarking suite

Want to contribute? PRs are welcome!

ğŸ¤ Contributing

We welcome contributions of all kinds.
See CONTRIBUTING.md for guidelines.

ğŸ“„ License

This project is licensed under the MIT License.
You are free to use, modify, and distribute it.

ğŸ“š Citation
@software{llm_evaluation_toolkit,
  title  = {LLM Evaluation Toolkit},
  author = {Adeyemi, Sam},
  year   = {2025},
  url    = {https://github.com/SamAde1203/llm-evaluation-toolkit}
}

â¤ï¸ Built for the AI Research & Engineering Community

This toolkit exists to make LLM evaluation transparent, reproducible, and scientifically rigorous â€” empowering anyone to build safer, more reliable AI systems.
